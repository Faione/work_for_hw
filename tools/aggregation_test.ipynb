{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55376952-7ebb-4e4d-b9c0-db6835b10ae8",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "\n",
    "## 场景一\n",
    "\n",
    "### (1) 描述\n",
    "\n",
    "场景一中，所有实验按固定的模式运行，包含干扰循环和负载循环\n",
    "1. 初始化 client 和 server(vm)\n",
    "2. 启动 background 干扰，当前支持的干扰有: cpu/cache/mem/io/net\n",
    "3. 启动 workload， 当前支持的workload有: redis/nginx/mysql\n",
    "4. workload 结束后，记录一份 `workload_info`, 包含时间戳, 负载、干扰强度等metadata\n",
    "5. 回到步骤3，执行下一个workload，当所有workload都执行完毕后，记录一份 `info_per_epoch`, 包含所有已经执行的 `workload_info`, 以及此次循环的干扰强度信息\n",
    "6. 回到步骤2，直到所有的强度的干扰都执行完毕\n",
    "7. 结束实验，记录实验的启动、结束时间，总共消耗的时间，以及干扰循环的总次数\n",
    "\n",
    "以上信息保存在 `exp.json` 或 `date.json` 中，另外通过 Prometheus client 请求得到整个实验周期中的metric数据，保存在 `merged.csv` 中\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9cf9eb-62af-423e-a82d-5601033ae7e0",
   "metadata": {},
   "source": [
    "### (2) 数据处理\n",
    "\n",
    "\n",
    "#### 1. Quick Start\n",
    "\n",
    "\n",
    "通过 `read_from_dir(dir)` 读取 metadata 和csv 数据创建 `ExpData` 实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fdea74-c28a-4812-9e9f-e60da97cca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aggregation import *\n",
    "pd.set_option('display.max_rows', 10) \n",
    "\n",
    "exp_root = \"/home/ict/appProfile/data/redis_1/no_stress/redis_no_20231102130306\"\n",
    "exp_data = read_from_dir(exp_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59e7b6-4047-4700-ace6-c8bccca88a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift time\n",
    "# 32 for opt, 10 for warmup, 1 for container start\n",
    "shift = exp_data._time_shift(opt_interval=32, delay=11)\n",
    "exp_data.shift_time(shift).exp[\"info_per_epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9699d-06c5-460a-a991-356ec12f76fe",
   "metadata": {},
   "source": [
    "调用 `agg_epoch()` 方法之后，`exp_data` 会按每次干扰循环聚合数据, 这也是推荐的使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebc3f8-a303-4159-8586-0db9e29841c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epoch = exp_data.agg_epoch()\n",
    "df_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfe74b-9899-42a7-9184-a5298c2f45fe",
   "metadata": {},
   "source": [
    "因为返回的仍然是一个 DataFrame, 因此后续可以根据数据处理的需要自行设置方法，如下展示一种从 `df_epoch` 数据中获取某个 workload 数据的流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172beca-1380-4822-b278-2d9238d12068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epoch_group = df_epoch.groupby(df_epoch.index)\n",
    "keys = list(df_epoch_group.groups.keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6cc30-513c-4ad9-80e2-8e845bbdd74c",
   "metadata": {},
   "source": [
    "使用 `groupby` 函数处理 Dataframe, 并从 `groups.keys()` 选择一个获取此 workload 的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3005aaf-d466-495f-a149-a4f06bddd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workload = df_epoch_group.get_group(keys[0])\n",
    "df_workload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f36eb-5f2d-4615-aa3d-1f3031609f96",
   "metadata": {},
   "source": [
    "#### 2. Custom Process\n",
    "\n",
    "`exp_data` 将读入的数据保存在 `exp` 与 `df` 两个字段中，可以通过 `exp_data.exp` 与 `exp_data.df` 来直接访问读入的数据， 如获取某个 workload info, 可以采取如下方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef9431-dd0c-4b9d-bd1f-c7b8628f6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "workload_info = exp_data.exp[\"info_per_epoch\"][0][\"workloads\"][keys[0]]\n",
    "workload_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6961c-6227-47b4-845e-3fba10681375",
   "metadata": {},
   "source": [
    "`workloads_of` 可以通过 name 获取所有的 workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fce27-a6a4-4777-bc43-0c7058e3c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "workload_infos = exp_data.workloads_of(keys[0])\n",
    "workload_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ee489-62d5-4487-a0b4-c0c851c28bc7",
   "metadata": {},
   "source": [
    "随后，可以使用” `workload_df` 来获取此 workload 对应的 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df87b2-90d6-4f36-93fb-822d91a5b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data.workload_df(workload_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb67da-7903-4b47-adc7-bb7266010527",
   "metadata": {},
   "source": [
    "默认情况下不会提取 \"stress\" 数据，可以指定参数 `with_stress` 来开启"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40f991-c33a-4aac-b127-4c5ced69c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data.workload_df(workload_info, with_stress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5211b53-ce90-4229-adcb-58eadab4ea78",
   "metadata": {},
   "source": [
    "或者通过 `agg_one_workload` 获取此 workload 对应时间序列下的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731642cb-d705-4084-ad95-ff474b21518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data.agg_one_workload(workload_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117f69f-3f88-484f-a60e-f21dc14594eb",
   "metadata": {},
   "source": [
    "`exp_data` 也允许只获取某个 干扰循环 的数据，通过下标指示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b33e0-f532-418f-b132-647719a1e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data.agg_epoch(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad7167-b9f2-4044-b6a1-8be3f84200de",
   "metadata": {},
   "source": [
    "### 3. Advanced Usage\n",
    "\n",
    "默认情况下 `exp_data` 对每个 workload 采用如下预处理手段，按顺序依次为:\n",
    "1. `filter_column_startswith(col_prefix=(\"vm\", \"app\"))`: 只选用 `vm`, `app` 前缀的指标\n",
    "2. `filter_column_useless(std_min=1e-10)`: 过滤掉平均值为0, 或方差小于 `1e-10` 的指标\n",
    "3. `filter_row_noise(col_prefix=(\"app\"))`: 过滤行中 `app` 为前缀指标中的离群值\n",
    "\n",
    "同时，对于每个 workload 采用如下聚合手段，按顺序依次为:\n",
    "1. `lambda x : x.mean().to_frame().T`: 将一个workload时序数据按均值压缩为一行\n",
    "\n",
    "`exp_data` 允许对上述处理进行自定义, 需要注意的是, 自定义的方法设置完毕之后，将会一直生效，包括在 `agg_epoch` 时\n",
    "- `set_workload_preprocess_funcs(df_funcs):`: 自定义预处理手段\n",
    "- `set_workload_agg_funcs(df_funcs):`: 自定义聚合手段\n",
    "\n",
    "其中 `df_funcs` 为一组函数，每个函数都满足如下签名:\n",
    "- `df_func(df: DataFrame) -> DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2478ddd-8dd8-4bbf-ade2-95783bf9ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defualt_workload_preprocess_funcs = [\n",
    "#     filter_column_startswith(col_prefix=(\"stress\", \"host\", \"vm\", \"app\")),\n",
    "#     filter_column_useless(excol_prefix=(\"stress\")),\n",
    "#     filter_row_noise(col_prefix=(\"app\")),\n",
    "# ]\n",
    "\n",
    "# defualt_workload_agg_funcs = [\n",
    "#     lambda x : x.mean().to_frame().T,\n",
    "# ]\n",
    "\n",
    "exp_data.set_workload_preprocess_funcs([\n",
    "    filter_column_startswith(col_prefix=(\"stress\", \"host\", \"vm\", \"app\")),\n",
    "    filter_column_useless(excol_prefix=(\"stress\")),\n",
    "])\n",
    "\n",
    "exp_data.workload_df(workload_info, with_stress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac88298-1e00-4134-abf2-ef54ae8cfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最大值而不是平均值\n",
    "exp_data.set_workload_agg_funcs([\n",
    "    lambda x : x.max().to_frame().T,\n",
    "])\n",
    "\n",
    "exp_data.agg_one_workload(workload_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af6dc3-6fc8-4c28-a3fb-d79f7f028ee5",
   "metadata": {},
   "source": [
    "### exp 运算\n",
    "\n",
    "`exp.json` 中以 workload 为粒度进行编排组织\n",
    "- `info_per_epoch` 以 epcoh 的维度组织 workload, 每个 epoch 在全局设置，如干扰上存在不同\n",
    "- `info_per_workload` 以 workload 的维度组织，每个 workload 中包含了同一个 workload 不同epoch下的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca5147-c4e9-4c9c-920f-3ecd89b8087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert exp_data.exp[\"info_per_workload\"] == ipe_to_ipw(exp_data.exp[\"info_per_epoch\"])\n",
    "assert exp_data.exp[\"info_per_epoch\"] == ipw_to_ipe(exp_data.exp[\"info_per_workload\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79510a-1aaf-4297-9540-2b99dc0ee1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = exp_data.exp\n",
    "keys = list(exp[\"info_per_workload\"].keys())\n",
    "sub_workload_1 = {k: exp[\"info_per_workload\"][k] for k in keys[:len(keys) // 2]}\n",
    "sub_workload_2 = {k: exp[\"info_per_workload\"][k] for k in keys[len(keys) // 2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e8794-d2c0-4055-939c-ecb50f88f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_to_exp(workload):\n",
    "    info_per_workload = workload\n",
    "    info_per_epoch = ipw_to_ipe(workload)\n",
    "    \n",
    "    workload_keys = list(info_per_workload.keys())\n",
    "    start_time = info_per_epoch[0][\"workloads\"][workload_keys[0]][\"start_time\"]\n",
    "    end_time = info_per_epoch[-1][\"workloads\"][workload_keys[-1]][\"end_time\"]\n",
    "    total_time = end_time - start_time\n",
    "    n_epoch = len(info_per_epoch)\n",
    "    date_format = 'timestamp'\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"total_time\": total_time,\n",
    "        \"n_epoch\": n_epoch,\n",
    "        \"date_format\": date_format,\n",
    "        \"info_per_workload\": info_per_workload,\n",
    "        \"info_per_epoch\": info_per_epoch,\n",
    "    }\n",
    "sub_exp_1 = workload_to_exp(sub_workload_1)\n",
    "sub_exp_2 = workload_to_exp(sub_workload_2)\n",
    "\n",
    "sub_data_1 = filter_row_timerange(\n",
    "    format_to_13_timestamp(sub_exp_1[\"start_time\"]),\n",
    "    format_to_13_timestamp(sub_exp_1[\"end_time\"]),\n",
    ")(exp_data.df)\n",
    "\n",
    "sub_data_2 = filter_row_timerange(\n",
    "    format_to_13_timestamp(sub_exp_2[\"start_time\"]),\n",
    "    format_to_13_timestamp(sub_exp_2[\"end_time\"]),\n",
    ")(exp_data.df)\n",
    "\n",
    "sub_exp_data_1 = ExpData(sub_data_1, sub_exp_1)\n",
    "sub_exp_data_2 = ExpData(sub_data_2, sub_exp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da482c2b-e9e4-42f8-a104-1bbd6ca197ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_exp_data = concat([sub_exp_data_1, sub_exp_data_2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
